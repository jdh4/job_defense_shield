<GREETING>

Below are your jobs over the past <DAYS> days which appear to be using more nodes
than necessary:

<TABLE>

The "Nodes" column shows the number of nodes used to run the job. The
"Nodes-Needed" column shows the minimum number of nodes needed to run the
job (these values are calculated based on the number of requested CPU-cores
while taking into account the CPU memory usage of the job). "Mem-per-Node"
is the mean CPU memory used per node.

When possible please try to minimize the number of nodes per job by using all
of the CPU-cores of each node. This will help to maximize the overall job
throughput of the cluster.

<CLUSTER> (<PARTITIONS>) is composed of nodes with <CPN> CPU-cores and <MPN> GB of CPU memory. If
your job requires <NUM-CORES> CPU-cores (and you do not have high memory demands) then
use, for example:

<SBATCH>

If you are unsure about the meanings of --nodes, --ntasks, --ntasks-per-node and
--cpus-per-task, see our Slurm webpage:

    https://researchcomputing.princeton.edu/support/knowledge-base/slurm

Additionally, see this general overview on parallel computing:

    https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code

It is very important to conduct a scaling analysis to find the optimal number
of nodes and CPU-cores to use for a given parallel job. The calculation of
"Nodes-Needed" above is based on your choice of the total CPU-cores which
may not be optimal. For information on conducting a scaling analysis:

    https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis

Consider attending an in-person Research Computing help session for assistance:

    https://researchcomputing.princeton.edu/support/help-sessions

Replying to this automated email will open a support ticket with Research
Computing.
