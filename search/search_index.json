{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Defend the Hardware","text":"<p>Job Defense Shield is a Python code for sending automated email alerts to users and for creating reports for system administrators. It is built on the Jobstats job monitoring platform.</p> <p>Automated email alerts to users are available for these cases:</p> <ul> <li>CPU or GPU jobs with 0% utilization (see email below)</li> <li>Heavy users with low mean CPU or GPU efficiency</li> <li>Jobs that allocate excess CPU memory (see email below)</li> <li>Jobs that allocate too many CPU-cores per GPU</li> <li>Jobs that allocate too much CPU memory per GPU</li> <li>Serial jobs that allocate multiple CPU-cores</li> <li>Users that routinely run with excessive time limits</li> <li>Jobs that could have used a smaller number of nodes</li> <li>Jobs that could have used less powerful GPUs</li> <li>Jobs thar ran on specialized nodes but did not need to</li> </ul> <p>All of the instances in the list above can be formulated as a report for system administrators. The most popular reports for system administrators are:</p> <ul> <li>A list of users (and their jobs) with the most GPU-hours at 0% utilization</li> <li>A list of the heaviest users with low CPU/GPU utilization</li> <li>A list of users that are over-allocating the most CPU memory</li> <li>A list of users that are over-allocating the most time</li> </ul> <p>The Python code is written using object-oriented programming techniques which makes it easy to create new alerts and reports.</p>"},{"location":"#example-reports","title":"Example Reports","text":"<p>Which users have wasted the most GPU-hours?</p> <pre><code>                         GPU-Hours at 0% Utilization                          \n------------------------------------------------------------------------------\n    User   GPU-Hours-At-0%  Jobs                 JobID                  emails\n------------------------------------------------------------------------------\n1  u12998        308         39   62266607,62285369,62303767,62317153+   1 (7)\n2  u9l487         84         14   62301196,62301737,62301738,62301742+   0     \n3  u39635         25          2                     62184669,62187323    0     \n4  u24074         24         13   62303161,62303182,62303183,62303184+   0      \n------------------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, pli-c, pli-p, pli, pli-lc\n     Start: Wed Feb 12, 2025 at 09:50 AM\n       End: Wed Feb 19, 2025 at 09:50 AM\n</code></pre> <p>Which users are wasting the most CPU memory?</p>"},{"location":"#example-emails","title":"Example Emails","text":"<p>Below is an example email for the automatic cancellation of a GPU job with 0% utilization:</p> <pre><code>Hi Alan (u1234),\n\nThe jobs below have been cancelled because they ran for nearly 2 hours at 0% GPU\nutilization:\n\n     JobID    Cluster  Partition    State    GPUs-Allocated GPU-Util  Hours\n    60131148   della      llm     CANCELLED         4          0%      2.0  \n    60131741   della      llm     CANCELLED         4          0%      1.9  \n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://your-institution.edu/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>Below is an example email to a user that is requesting too much CPU memory:</p> <pre><code>Hi Alan (u1234),\n\nBelow are your jobs that ran on BioCluster in the past 7 days:\n\n     JobID   Memory-Used  Memory-Allocated  Percent-Used  Cores  Hours\n    5761066      2 GB          100 GB            2%         1     48\n    5761091      4 GB          100 GB            4%         1     48\n    5761092      3 GB          100 GB            3%         1     48\n\nIt appears that you are requesting too much CPU memory for your jobs since\nyou are only using on average 3% of the allocated memory. For help on\nallocating CPU memory with Slurm, please see:\n\n    https://your-institution.edu/knowledge-base/memory\n\nReplying to this automated email will open a support ticket with Research\nComputing. \n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>The software has a <code>check</code> mode that shows on which days a given user received an alert of a given type. Users that appear to be ignoring the email alerts can be contacted directly. Emails to users are most effective when sent sparingly. For this reason, there is a command-line parameter to specify the amount of time that must pass before the user can receive another email of the same nature.</p> <p>The example below shows how the script is called to notify users in the top N by usage with low CPU or GPU efficiencies over the last week:</p> <pre><code>$ job_defense_shield --low-xpu-efficiencies --days=7 --email\n</code></pre> <p>The default thresholds are 60% and 15% for CPU and GPU utilization, respectively, and N=15.</p> <p>There is a corresponding entry in the configuration file:</p> <pre><code>low-xpu-efficiencies:\n  cluster:\n    - della\n  partitions:\n    - all\n  threshold: 10\n</code></pre>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Summary statistics for each completed job are stored in a compressed format in the <code>AdminComment</code> field in the Slurm database. The software described here works by calling the Slurm <code>sacct</code> command while requesting several fields including <code>AdminComment</code>. The <code>sacct</code> output is stored in a <code>pandas</code> dataframe for processing.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Job Defense Shield is configured using a YAML file, which specifies global settings as well as the individual alerts to run.</p> <p>Below is a minimal configuration file (<code>config.yaml</code>) with one alert:</p> <pre><code>%YAML 1.1\n---\n#####################\n## GLOBAL SETTINGS ##\n#####################\njobstats-module-path: /path/to/jobstats/\nviolation-logs-path: /path/to/violations/\nemail-files-path: /path/to/email/\nemail-domain-name: \"@institution.edu\"\nsender: support@institution.edu\nreply-to: support@institution.edu\ngreeting-method: getent\nworkday-method: file\nholidays-file: /path/to/holidays.txt\nreport-emails:\n  - admin1@institution.edu\n  - admin2@institution.edu\n\n\n##################################\n## ZERO CPU UTILIZATION (ALERT) ##\n##################################\nzero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n    - bigmem\n  min_run_time: 61 # minutes\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - job-alerts-aaaalegbihhpknikkw2fkdx6gi@institution.slack.com\n    - admin@institution.edu\n</code></pre> <p>See a full example of config.yaml in the GitHub repository.</p>"},{"location":"configuration/#global-settings","title":"Global Settings","text":""},{"location":"configuration/#jobstats-files","title":"Jobstats Files","text":"<p>The first entry in <code>config.yaml</code> is the path to <code>jobstats.py</code> and <code>config.py</code>:</p> <pre><code>jobstats-module-path: /path/to/jobstats/\n</code></pre> <p>These two files are needed for getting the summary statistics of actively running jobs. To be clear, <code>/path/to/jobstats</code> should contain these files:</p> <pre><code>/path/to/jobstats/jobstats.py\n/path/to/jobstats/config.py\n</code></pre> <p>The value of <code>PROM_SERVER</code> is taken from <code>config.py</code>.</p> <p>If you are not interested in addressing the underutilization of actively running jobs then just set this to a dummy path (e.g., <code>/tmp</code>).</p>"},{"location":"configuration/#violation-logs","title":"Violation Logs","text":"<p>One must specifiy the path to a writable directory to store the underutilization history of each user:</p> <pre><code>violation-logs-path: /path/to/violations/\n</code></pre> <p>The files stored in this directory are read when deciding whether or not sufficient time has passed to send the user another email. These files are important and should be backed up.</p>"},{"location":"configuration/#email-settings","title":"Email Settings","text":"<p>Set the path to your email files. These are the files that are read and enhanced by the software. A set of examples in found in the <code>email</code> directory of the <code>job_defense_shield</code> GitHub repository.</p> <pre><code>email-files-path: /path/to/email/\n</code></pre> <p>Specify the email domain for your institution:</p> <pre><code>email-domain-name: \"@institution.edu\"\n</code></pre> <p>Usernames will be concatenated with the email domain to make email addresses.</p> <p>Specify the <code>sender</code> and <code>reply-to</code> values for sending emails:</p> <pre><code>sender: support@institution.edu\nreply-to: support@institution.edu\n</code></pre> <p>Tip</p> <p>By using a <code>reply-to</code> that is different from <code>sender</code>, one can prevent auto-reply or out-of-office emails from creating new support tickets.</p> <p>Use the <code>greeting-method</code> to determine the first line of the email that users receive:</p> <pre><code>greeting-method: getent\n</code></pre> <p>If you find that <code>getent</code> is not working properly during testing then use <code>basic</code>.</p> <p>Lastly, one can create multiple reports and have those sent to administrators by email when the <code>--report</code> flag is used:</p> <pre><code>report-emails:\n  - admin1@institution.edu\n  - admin2@institution.edu\n</code></pre>"},{"location":"configuration/#workdays","title":"Workdays","text":"<p>Email alerts are only sent to users on workdays. Pick a method to distinguish the workdays from weekends and holidays. The most flexible method is <code>file</code>:</p> <pre><code>workday-method: file\nholidays-file: /path/to/holidays.txt\n</code></pre> <p>The file <code>holidays.txt</code> should be a list of dates with the format YYYY-MM-DD:</p> <pre><code>$ cat holidays.txt\n2025-05-26\n2025-06-19\n2025-07-04\n</code></pre> <p>If you only want to avoid weekends and U.S. Federal holidays then use:</p> <pre><code>workday-method: usa\n</code></pre> <p>If every day is a workday then:</p> <pre><code>workday-method: always\n</code></pre> <p>The <code>cron</code> setting can be used to avoid weekends so really this section is about dealing with holidays.</p>"},{"location":"configuration/#other-settings","title":"Other Settings","text":"<p>Partition names can be renamed:</p> <pre><code>partition-renamings:\n  datascience: datasci\n</code></pre> <p>If a partition is renamed then the new name must be used throughout the configuration file.</p> <p>For users that do not use their institutional email address, one can specify external addresses:</p> <pre><code>external-emails:\n  u12345: alan.turing@gmail.com\n  u23456: einstein@yahoo.com\n</code></pre>"},{"location":"configuration/#specifying-a-custom-configuration-file","title":"Specifying a Custom Configuration File","text":"<p>By default the software will look for <code>config.yaml</code> in the same directory as <code>job_defense_shield.py</code>. One can override this behavior by using the <code>--config-file</code> option:</p> <pre><code>$ job_defense_shield --config-file=/path/to/myconfig.yaml --low-gpu-efficiency\n</code></pre> <p>The ability to use different configuration files provides additional flexibility. For instance, for some institutions it may make sense to have a different configuration file for each cluster or for different alerts.</p>"},{"location":"configuration/#each-alert-must-have-a-different-name","title":"Each Alert Must Have a Different Name","text":"<p>Consider the following two alerts (pay attention to the alert names):</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n\nzero-cpu-utilization-1:\n  cluster: della\n  partitions:\n    - physics\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>While the two alerts above are written for different clusters, only the second one will run since both alerts have the same name (<code>zero-cpu-utilization-1</code>).</p> <p>Danger</p> <p>Make sure each alert name has a different number at the end. An alert with the same name as one previously defined will override the previous alert.</p> <p>The corrected version would be:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n\nzero-cpu-utilization-2:\n  cluster: della      \n  partitions:\n    - physics\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The second alert now has the name <code>zero-cpu-utilization-2</code>.</p>"},{"location":"configuration/#include-or-fully-remove-a-setting","title":"Include or Fully Remove a Setting","text":"<p>There are many optional settings for each alert. If you do not want to use an optional setting then fully remove the line.</p> <p>The following is incorrect for <code>min_run_time</code>:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  min_run_time:\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>If the default value for <code>min_run_time</code> should be used then completely remove the line. Here is the correct entry:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>Another correct way is to specify the value:</p> <pre><code>zero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  min_run_time: 0  # minutes\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre>"},{"location":"configuration/#full-example-configuration-file","title":"Full Example Configuration File","text":"<p>For more examples see config.yaml in the GitHub repository.</p>"},{"location":"configuration/#writing-and-testing-custom-emails","title":"Writing and Testing Custom Emails","text":"<p>See the next section to learn about sending custom emails to your users.</p>"},{"location":"contributions/","title":"Contributions","text":"<p>Contributions to the Jobstats platform and its tools are welcome. To work with the code, build a Conda environment:</p> <pre><code>$ conda create --name jds-env python=3.12     \\\n                              pandas          \\\n                              pyarrow         \\\n                              pytest-mock     \\\n                              ruff            \\\n                              blessed         \\\n                              requests        \\\n                              pyyaml          \\\n                              mkdocs-material \\\n                              -c conda-forge -y\n</code></pre>"},{"location":"contributions/#testing","title":"Testing","text":"<p>Be sure that the tests are passing before making a pull request:</p> <pre><code>(jds-env) $ pytest\n</code></pre>"},{"location":"contributions/#static-checking","title":"Static Checking","text":"<p>Run <code>ruff</code> and make sure it is passing for each source file modified:</p> <pre><code>(jds-env) $ ruff check myfile.py\n</code></pre>"},{"location":"contributions/#documentation","title":"Documentation","text":"<p>The documentation is generated with Material for MkDocs. To build and serve the documentation:</p> <pre><code>(jds-env) $ mkdocs build\n(jds-env) $ mkdocs serve\n# open a web browser\n</code></pre>"},{"location":"emails/","title":"Emails","text":""},{"location":"emails/#custom-emails-to-users","title":"Custom Emails to Users","text":"<p>Each alert requires a text file for the <code>email_file</code>:</p> <pre><code>##################################\n## ZERO CPU UTILIZATION (ALERT) ##\n##################################\nzero-cpu-utilization-1:\n  cluster: stellar\n  partitions:\n    - cpu\n  email_file: \"zero_cpu_utilization.txt\"\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The location of the <code>email_file</code> is set in <code>config.yaml</code> by:</p> <pre><code>email-files-path: /path/to/email/\n</code></pre> <p>Here is an example <code>email_file</code>:</p> <pre><code>$ cat /path/to/email/zero_cpu_utilization.txt\n&lt;GREETING&gt;\n\nBelow are your recent jobs that did not use all of the allocated nodes:\n\n&lt;TABLE&gt;\n\nThe CPU utilization was found to be 0% on each of the unused nodes. You can see\nthis by running the \"jobstats\" command, for example:\n\n&lt;JOBSTATS&gt;\n\nPlease investigate the reason(s) that the code is not using all of the allocated\nnodes before running additional jobs.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>There are three \"tags\" in the text file above: <code>&lt;GREETING&gt;</code>, <code>&lt;TABLE&gt;</code> and <code>&lt;JOBSTATS&gt;</code>. Each tag will be replaced by the corresponding value in Python when creating the email. The resulting email will appear as:</p> <pre><code>Hello Alan (u12345),\n\nBelow are your recent jobs that did not use all of the allocated nodes:\n\n      JobID    Cluster  Nodes  Nodes-Unused CPU-Util-Unused  Cores  Hours\n    62734245    della     4          3             0%          12    2.3 \n    62734246    della     6          5             0%          12    2.4 \n\nThe CPU utilization was found to be 0% on each of the unused nodes. You can see\nthis by running the \"jobstats\" command, for example:\n\n    $ jobstats 62734245\n\nPlease investigate the reason(s) that the code is not using all of the allocated\nnodes before running additional jobs.\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>Tags can be placed anywhere in your <code>email_file</code>. For example, one can include a tag in the middle of a sentence:</p> <pre><code>Below are your jobs on &lt;CLUSTER&gt; that did not use all of the allocated nodes:\n</code></pre> <p>Each alert has a finite set of tags that may be used to generate custom emails. There are a set of example email files in the <code>email</code> directory of the GitHub repository. It is recommended that you copy these and modify them as you see fit. It might also be a good idea to put them under version control along with <code>config.yaml</code> and <code>holidays.txt</code>.</p>"},{"location":"emails/#testing-the-sending-of-emails-to-users","title":"Testing the Sending of Emails to Users","text":"<p>If <code>config.yaml</code> exists, an administrator can see the output of an alert by running it:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiency\n</code></pre> <p>One adds the <code>--email</code> flag to send emails to users:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiency --email\n</code></pre> <p>For testing, one can add a second flag that will only send the emails to <code>admin_emails</code> and not the users:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiency --email --no-emails-to-users\n</code></pre> <p>The <code>--no-emails-to-users</code> will also prevent violation log files from being updated. This allows administrators to test and modify the email messages in safety.</p> <p>There is one alert that requires one extra step, which is Cancel 0% GPU Jobs. In this case, one should add the following to the alert definition:</p> <pre><code>  do_not_cancel: True\n</code></pre>"},{"location":"nodelist/","title":"Nodelist","text":"<p>For certain systems, filtering jobs by <code>cluster</code> and <code>partitions</code> is insufficient. For instance, if the nodes on a given partition have different hardware specifications.</p> <p>In this case, more control is available by specifying a <code>nodelist</code>. Consider the following alert for identifying jobs that are allocating too much CPU memory per GPU:</p> <pre><code>too-much-cpu-mem-per-gpu-1:\n  cluster: della\n  partitions:\n    - gpu\n  cores_per_node: 48\n  gpus_per_node: 4\n  cpu_mem_per_node: 1000       # GB\n  cpu_mem_per_gpu_target: 240  # GB\n  cpu_mem_per_gpu_limit: 250   # GB\n  email_file: \"too_much_cpu_mem_per_gpu_2.txt\"\n  nodelist:\n    - della-l01g1\n    - della-l01g2\n    - della-l01g3\n    - della-l01g4\n    - della-l01g5\n    - della-l01g6\n    - della-l01g7\n    - della-l01g8\n</code></pre> <p>The alert above will first filter on <code>cluster</code>, then <code>partitions</code>, and, finally, it will only consider jobs that ran exclusively on one or more nodes in the <code>nodelist</code>. This makes it possible to write alerts for partitions composed of heterogeneous hardware (e.g., a mix a GPUs with different GPU memory).</p>"},{"location":"publications/","title":"Publications","text":"<ul> <li>PEARC 2024 poster: PDF</li> <li>PEARC 2023 talk: PDF</li> <li>PEARC 2023 paper: PDF</li> </ul>"},{"location":"setup/","title":"Installation","text":"<p>We assume that the Jobstats platform is available and working. While there are some alerts that do not require Jobstats (e.g., time, utilization-overview), to take full advantage of the software it should be present.</p> <p>The installation requirements for Job Defense Shield are Python 3.6+ and version 1.2+ of the Python <code>pandas</code> package. The <code>jobstats</code> command is also required if one wants to examine actively running jobs such as when looking for jobs with zero GPU utilization. The Python code, example alerts and emails, and instructions are available in the GitHub repository.</p> <pre><code>$ conda create --name jds-env pandas pyarrow blessed requests pyyaml -c conda-forge -y\n</code></pre> <pre><code>$ apt-get install python3-pandas python3-requests python3-yaml python3-blessed\n</code></pre> <p>One only needs <code>requests</code>, <code>blessed</code> and <code>jobstats.py</code> to get the data for actively running jobs.</p>"},{"location":"setup/#testing-the-installation","title":"Testing the Installation","text":"<p>To test the software, run this simple command which does not send any emails:</p> <pre><code>$ job_defense_shield --utilization-overview\n</code></pre> <p>The command above will show an overview of the number of CPU-hours and GPU-hours across all clusters and partitions in the Slurm database.</p> <p>Here is an example:</p> <pre><code>$ job_defense_shield.py --utilization-overview\n\n\n           Utilization Overview           \n------------------------------------------\ncluster   users   cpu-hours    gpu-hours  \n------------------------------------------\n   della  491   1664010 (16%) 107435 (76%)\n stellar  148   6290093 (61%)    3193 (2%)\n   tiger   43   2276967 (22%)       0 (0%)\ntraverse    1     126183 (1%)  31546 (22%)\n\n\n            Utilization Overview by Partition            \n---------------------------------------------------------\ncluster    partition    users   cpu-hours    gpu-hours  \n---------------------------------------------------------\n   della           cpu  296    963959 (58%)       0 (0%)\n   della         pli-c   27    217984 (13%)  31254 (29%)\n   della    gpu-shared  116     158050 (9%)  42930 (40%)\n   della       datasci   45      95059 (6%)       0 (0%)\n   della       physics   16      66573 (4%)       0 (0%)\n   della           gpu   35      63406 (4%)  11736 (11%)\n   della        cryoem   20      24510 (1%)    4524 (4%)\n   della         donia    6      19055 (1%)       0 (0%)\n   della        gpu-ee    3      18779 (1%)     255 (0%)\n   della           pli   23      16976 (1%)    7806 (7%)\n   della       gputest  115      11698 (1%)    1485 (1%)\n   della           mig   47       7445 (0%)    7445 (7%)\n   della         malik    1        515 (0%)       0 (0%)\n   della     gpu-wentz    1          3 (0%)       1 (0%)\n stellar            pu   48   2998473 (48%)       0 (0%)\n stellar          pppl   38   1747524 (28%)       0 (0%)\n stellar         cimes   23   1423071 (23%)       0 (0%)\n stellar        serial   44      60886 (1%)       0 (0%)\n stellar           all   55      45176 (1%)       0 (0%)\n stellar           gpu   22      14691 (0%)  3193 (100%)\n stellar        bigmem    3        270 (0%)       0 (0%)\n   tiger           cpu   31   1792892 (79%)            0\n   tiger           ext    5    476715 (21%)            0\n   tiger        serial   15       7360 (0%)            0\ntraverse           all    1   126183 (100%) 31546 (100%)\n</code></pre>"},{"location":"setup/#email-test","title":"Email Test","text":"<p>Make a <code>config.yaml</code> file as below and place it in the same location as <code>job_defense_shield.py</code>:</p> <pre><code>jobstats-module-path: /tmp\nviolation-logs-path: /tmp\nemail-files-path: /tmp\nemail-domain-name: \"@institution.edu\"\nsender: support@institution.edu\nreply-to: support@institution.edu\nreport-emails:\n  - admin1@institution.edu\n</code></pre> <p>Now try again and you should received the report by email:</p> <pre><code>$ job_defense_shield.py --utilization-overview --report\n</code></pre>"},{"location":"setup/#troubleshooting-the-installation","title":"Troubleshooting the Installation","text":"<p>Finding the right <code>python</code></p> <pre><code>$ python -c \"import pandas\"\n$ python -c \"import pyyaml\"\n</code></pre> <p>Finding the configuration file. The code will look for the configuration file in multiple locations. You can always specify the full path with:</p> <pre><code>--cfg=&lt;path/to&gt;/job_defense_shield/config.yaml\n</code></pre>"},{"location":"setup/#creating-a-configuration-file-for-production","title":"Creating a Configuration File for Production","text":"<p>See the next section to learn how to write a proper configuration file.</p>"},{"location":"support/","title":"Support","text":"<p>For assistance with Job Defense Shield, please post an issue on the GitHub repository: https://github.com/PrincetonUniversity/job_defense_shield</p> <p>Please include the versions of Python and pandas that you are using.</p> <p>For assistance with the Jobstats platform, please post an issue on the Jobstats GitHub repository: https://github.com/PrincetonUniversity/jobstats</p>"},{"location":"alert/cancel_gpu_jobs/","title":"Cancel GPU Jobs at 0% Utilization","text":"<p>This is one of the most popular features of Jobstats. This alert automatically cancels jobs with GPUs at 0% utilization. Up to two warning emails can be sent before each job is cancelled.</p> <p>Elevated Privileges</p> <p>This alert is different than the others in that it must be ran as a user with sufficient privileges to call <code>scancel</code>.</p>"},{"location":"alert/cancel_gpu_jobs/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for the configuration file where many of the settings are applied:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  second_warning_minutes: 105  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning:  \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_second_warning: \"cancel_gpu_jobs_warning_2.txt\"\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  max_interactive_hours: 8\n  max_interactive_gpus: 1\n  do_not_cancel: False\n  warnings_to_admin: True\n  admin_emails:\n    - admin@institution.edu\n  excluded_users:\n    - u12345\n    - u23456\n</code></pre> <p>The settings are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. per alert.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions.</p> </li> <li> <p><code>sampling_period_minutes</code>: Number of minutes between executions of this alert. This number must also be the same in <code>cron</code> (see cron section below) or the scheduler that is used.</p> </li> <li> <p><code>first_warning_minutes</code>: (Optional) Number of minutes that the job must run before the first warning email can be sent.</p> </li> <li> <p><code>second_warning_minutes</code>: (Optional) Number of minutes that the job must run before the second warning email can be sent.</p> </li> <li> <p><code>cancel_minutes</code>: (Required) Number of minutes that the job must run before it can be cancelled.</p> </li> <li> <p><code>email_file_first_warning</code>: (Optional) File to be used for the first warning email.</p> </li> <li> <p><code>email_file_second_warning</code>: (Optional) File to be used for the second warning email.</p> </li> <li> <p><code>email_file_cancel</code>: (Required) File to be used for the cancellation email.</p> </li> <li> <p><code>jobid_cache_path</code>: (Optional) Path to a writable directory where a cache file containing the <code>jobid</code> of each job known to be using the GPUs is stored. This is a binary file with the name <code>.jobid_cache.pkl</code>. Including this setting will eliminate redundant calls to the Prometheus server.</p> </li> <li> <p><code>max_interactive_hours</code>: (Optional) An interactive job will only be cancelled if the run time limit is greater than <code>max_interactive_hours</code> and the number of allocated GPUs is less than or equal to <code>max_interactive_gpus</code>. Remove these lines if interactive jobs should not receive special attention. An interactive job is one with a <code>jobname</code> that starts with either <code>interactive</code> or <code>sys/dashboard</code>.</p> </li> <li> <p><code>max_interactive_gpus</code>: (Optional) See line above.</p> </li> <li> <p><code>gpu_frac_threshold</code>: For a given job, let <code>g</code> be the ratio of GPUs with non-zero utilization to the number of allocated GPUs. Jobs with <code>gpu_frac_threshold</code> greater than or equal to <code>g</code> will be excluded. For example, if <code>gpu_frac_threshold</code> is 0.8 and a job uses 7 of the 8 allocated GPUs then it will be excluded since 7/8 &gt; 0.8. Default: 1.0</p> </li> <li> <p><code>nodelist</code>: (Optional) Only apply this alert to jobs that ran on the specified nodes. See example.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of usernames to exclude from this alert.</p> </li> <li> <p><code>do_not_cancel</code>: (Optional) If <code>True</code> then <code>scancel</code> will not be called. This is useful for testing only. In this case, one should call the alert with <code>--email --no-emails-to-users</code>. Default: <code>False</code></p> </li> <li> <p><code>warnings_to_admin</code>: (Optional) If <code>False</code> then warning emails will not be sent to <code>admin_emails</code>. Only cancellation emails will be sent. Default: <code>True</code></p> </li> <li> <p><code>admin_emails</code>: (Optional) List of administrator email addresses that should receive the warning and cancellation emails that are sent to users.</p> </li> </ul> <p>Note that jobs are not cancelled at after exactly <code>cancel_minutes</code> since the alert is only called every N minutes via cron. The same is true for warning emails.</p> <p>In Jobstats, a GPU is said to have 0% utilization if all of the measurements made by the NVIDIA exporter are zero. Measurements are typically made every 30 seconds or so. For the actual value see <code>SAMPLING_PERIOD</code> in <code>config.py</code> for Jobstats.</p>"},{"location":"alert/cancel_gpu_jobs/#example-configurations","title":"Example Configurations","text":"<p>The example below will send one warning email and then cancel the jobs:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  first_warning_minutes:   60  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_first_warning:  \"cancel_gpu_jobs_warning_1.txt\"\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre> <p>The example below will send no warnings but only cancel jobs after 120 minutes:</p> <pre><code>cancel-zero-gpu-jobs-1:\n  cluster:\n    - della\n  partitions:\n    - gpu\n    - llm\n  sampling_period_minutes: 15  # minutes\n  cancel_minutes:         120  # minutes\n  email_file_cancel:         \"cancel_gpu_jobs_scancel_3.txt\"\n  jobid_cache_path: /path/to/writable/directory/\n  admin_emails:\n    - admin@institution.edu\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#testing","title":"Testing","text":"<p>For testing, be sure to use:</p> <pre><code>  do_not_cancel: True\n</code></pre> <p>Additionally, add the <code>--no-emails-to-users</code> flag:</p> <pre><code>$ python job_defense_shield.py --cancel-zero-gpu-jobs --email --no-emails-to-users\n</code></pre> <p>Learn more about email testing.</p>"},{"location":"alert/cancel_gpu_jobs/#first-warning-email","title":"First Warning Email","text":"<p>Below is an example email for the first warning (see <code>email/cancel_gpu_jobs_warning_1.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nYou have GPU job(s) that have been running for nearly 1 hour but appear to not\nbe using the GPU(s):\n\n       JobID    Cluster Partition  GPUs-Allocated  GPUs-Unused GPU-Util  Hours\n     60131148    della     gpu            4             4         0%       1  \n\nYour jobs will be AUTOMATICALLY CANCELLED if they are found to not be using the\nGPUs for 2 hours.\n\nPlease consider cancelling the job(s) listed above by using the \"scancel\"\ncommand:\n\n     $ scancel 60131148\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#tags","title":"Tags","text":"<p>These tags can be used to generate custom emails:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert (i.e., <code>partitions</code>).</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;MINUTES-1ST&gt;</code>: Number of minutes before the first warning is sent (<code>first_warning_minutes</code>).</li> <li><code>&lt;HOURS-1ST&gt;</code>: Number of hours before the first warning is sent.</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#second-warning-email","title":"Second Warning Email","text":"<p>Below is an example email for the second warning (see <code>email/cancel_gpu_jobs_warning_2.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nThis is a second warning. The jobs below will be cancelled in about 15 minutes\nunless GPU activity is detected:\n\n       JobID    Cluster Partition  GPUs-Allocated  GPUs-Unused GPU-Util  Hours\n     60131148    della     gpu           4             4          0%      1.6  \n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#tags_1","title":"Tags","text":"<p>These tags can be used to generate custom emails:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert (i.e., <code>partitions</code>).</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;MINUTES-1ST&gt;</code>: Number of minutes before the first warning is sent (<code>first_warning_minutes</code>).</li> <li><code>&lt;MINUTES-2ND&gt;</code>: Number of minutes before the second warning is sent (<code>second_warning_minutes</code>).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#cancellation-email","title":"Cancellation Email","text":"<p>Below is an example email (see <code>email/cancel_gpu_jobs_scancel_3.txt</code>):</p> <pre><code>Hi Alan (aturing),\n\nThe jobs below have been cancelled because they ran for more than 2 hours at 0% GPU\nutilization:\n\n     JobID    Cluster  Partition    State    GPUs-Allocated GPU-Util  Hours\n    60131148   della      gpu     CANCELLED         4          0%      2.1\n\nSee our GPU Computing webpage for three common reasons for encountering zero GPU\nutilization:\n\n    https://your-institution.edu/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/cancel_gpu_jobs/#tags_2","title":"Tags","text":"<p>These tags can be used to generate custom emails:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting generated by <code>greeting-method</code>.</li> <li><code>&lt;CLUSTER&gt;</code>: The cluster specified for the alert (i.e., <code>cluster</code>).</li> <li><code>&lt;PARTITIONS&gt;</code>: The partitions listed for the alert (i.e., <code>partitions</code>).</li> <li><code>&lt;SAMPLING&gt;</code>: The sampling period in minutes (<code>sampling_period_minutes</code>).</li> <li><code>&lt;CANCEL-MIN&gt;</code>: Number of minutes a job must run for before being cancelled (<code>cancel_minutes</code>).</li> <li><code>&lt;CANCEL-HRS&gt;</code>: Number of hours a job must run for before being cancelled.</li> <li><code>&lt;TABLE&gt;</code>: Table of job data.</li> <li><code>&lt;JOBSTATS&gt;</code>: <code>jobstats</code> command for the first JobID (<code>$ jobstats 12345678</code>).</li> <li><code>&lt;SCANCEL&gt;</code>: <code>scancel</code> command for the first JobID (<code>$ scancel 12345678</code>).</li> </ul>"},{"location":"alert/cancel_gpu_jobs/#cron","title":"<code>cron</code>","text":"<p>Below is an example crontab for this alert:</p> <pre><code>PY=/var/spool/slurm/cancel_zero_gpu_jobs/envs/jds-env/bin\nJDS=/var/spool/slurm/job_defense_shield\nMYLOG=/var/spool/slurm/cancel_zero_gpu_jobs/log\nVIOLATION=/var/spool/slurm/job_defense_shield/violations\nMAILTO=admin@institution.edu\n\n*/15 * * * * ${PY}/python ${JDS}/job_defense_shield.py --cancel-zero-gpu-jobs --email -M della -r gpu &gt; ${MYLOG}/zero_gpu_utilization.log 2&gt;&amp;1\n</code></pre> <p>Note that the alert is ran every 15 minutes. This must also be the value of <code>sampling_period_minutes</code>.</p>"},{"location":"alert/cancel_gpu_jobs/#report","title":"Report","text":"<p>There is no report for this alert. To find out which users have the most GPU-hours at 0% utilization, see this alert. If you are automatically cancelling GPU jobs then no users should be able to waste significant resources.</p>"},{"location":"alert/cpu_fragmentation/","title":"Multinode CPU Fragmentation","text":"<p>Consider a cluster with 64 CPU-cores per node. A user can run a job that requires 128 CPU-cores by (1) allocating 64 CPU-cores on 2 nodes or (2) allocating 4 CPU-cores on 32 nodes. The former is in general strongly preferred. This alert catches jobs doing the latter, i.e., multinode jobs that allocate less than the number of available CPU-cores per node (e.g., 4 CPU-cores on 32 nodes). The memory usage of each job is taken into account when looking for fragmentation.</p> <p>Jobs with 0% CPU utilization on a node are ignored since those will captured by a different alert.</p>"},{"location":"alert/cpu_fragmentation/#report-for-system-administrators","title":"Report for System Administrators","text":"<pre><code>$ python job_defense_shield --multinode-cpu-fragmentation\n</code></pre>"},{"location":"alert/cpu_fragmentation/#email","title":"Email","text":"<p>Below is an example:</p>"},{"location":"alert/cpu_fragmentation/#configuration-file","title":"Configuration File","text":"<p>Below is the minimal settings for this alert:</p>"},{"location":"alert/cpu_fragmentation/#usage","title":"Usage","text":"<p>Use it</p>"},{"location":"alert/custom/","title":"Custom Email Alerts","text":"<p>This section explains who to add a custom alert four your institution.</p> <p>Copy the most similar existing alert:</p> <pre><code>--custom-1\n--custom-2\n--custom-3\n</code></pre> <p>CustomAlert(Base)</p> <pre><code>$ cd job_defense_shield/alert\n$ cp &lt;most-similar-alert&gt;.py CustomAlert1.py\n</code></pre> <p>Use a text editor to write the alert.</p> <p>Create an new entry in the configuration file:</p> <pre><code>custom-alert-1:\n  clusters:\n    - della\n  partitions:\n    - cpu\n    - gpu\n    - bigmem\n</code></pre> <p>Then run with:</p> <pre><code>$ job_defense_shield --custom-1 --days=10 --email\n</code></pre> <p>You can add any attribute to config.yaml and it will be available in the alert.</p>"},{"location":"alert/excess_cpu_memory/","title":"Excess CPU Memory","text":"<p>This alert sends emails to users that are allocating excess CPU memory. For example, the job used 1 GB but the user allocated 100 GB.</p>"},{"location":"alert/excess_cpu_memory/#configuration-file","title":"Configuration File","text":"<pre><code>excess-cpu-memory:\n  clusters:\n    - della\n  partition:\n    - cpu\n  cores_per_node: 28\n  tb_hours_per_day: 10\n  ratio_threshold: 0.35\n  mean_ratio_threshold: 0.35\n  median_ratio_threshold: 0.35\n  num_top_users: 10\n  combine_partitions: False\n  excluded_users:\n    - aturing\n    - einstein\n    - vonholdt\n  admin_emails:\n    - alerts-jobs-aaaalegbihhpknikkw2fkdx6gi@princetonrc.slack.com\n    - halverson@princeton.edu\n</code></pre> <p>Each configuration parameter is explained below:</p> <ul> <li> <p><code>cores_per_node</code>: Number of CPU-cores per node.</p> </li> <li> <p><code>tb_hours_per_day</code>: The threshold for TB-hours per day. This value is multiplied by <code>--days</code> to determine the threshold of TB-hours for the user to receive an email message.</p> </li> <li> <p><code>ratio_threshold</code>: This quantity is the sum of CPU memory used divded by the total memory allocated for all jobs of a given user in the specified time window. This quantity varies between 0 and 1.</p> </li> <li> <p><code>median_ratio_threshold</code>: This is median value of memory used divided by memory allocated for the individual jobs of the user.</p> </li> <li> <p><code>mean_ratio_threshold</code>: The mean value of the memory used divided by the memory allocated per job for a given user. This quantity varies between 0 and 1.</p> </li> </ul>"},{"location":"alert/excess_cpu_memory/#usage","title":"Usage","text":""},{"location":"alert/gpu_fragmentation/","title":"Multinode GPU Fragmentation","text":"<p>Consider a cluster with 4 GPUs per node. A user can run a job with 8 GPUs by either (1) allocating 4 GPUs on 2 nodes or (2) allocating 1 GPU on 8 nodes. The former is in general strongly preferred. This alert catches jobs doing the latter, i.e., multinode jobs that allocate less than the number of available GPUs per node (e.g., 1 GPU on 8 nodes).</p> <p>Jobs with a GPU at 0% utilization are ignored since they will be captured by the <code>--zero-gpu-utilization</code> alert.</p>"},{"location":"alert/gpu_fragmentation/#report","title":"Report","text":"<p>Here is example output of running the alert:</p> <pre><code>$ python job_defense_shield.py --multinode-gpu-fragmentation\ntable\n</code></pre>"},{"location":"alert/gpu_fragmentation/#email","title":"Email","text":"<pre><code>Hello Alan (u12345),\n\nBelow are jobs that ran on Della in the past 7 days that used 1 GPU per node\nover multiple nodes:\n\n     JobID     User   GPUs  Nodes GPUs-per-Node  Hours State GPU-eff\n    60969293 aturing   4     2          2          24    TO     0%  \n\nThe GPU nodes on Della have either 8 GPUs per node. For future jobs,\nplease try to use as few nodes as possible by allocating more GPUs per node. This\nis done by modifying the --gres Slurm directive as explained here:\n\n    https://researchcomputing.princeton.edu/support/knowledge-base/slurm#gpus\n\nIn any of your jobs have a low GPU utilization then please consider using only\na single GPU per job to improve efficiency.\n\nFor more information about the Della GPU nodes:\n\n    https://researchcomputing.princeton.edu/systems/della#gpus\n\nConsider attending an in-person Research Computing help session for assistance:\n\n    https://researchcomputing.princeton.edu/support/help-sessions\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre>"},{"location":"alert/gpu_fragmentation/#configuration-file","title":"Configuration File","text":"<pre><code>multinode-gpu-fragmentation-2:\n  cluster: della\n  partitions:\n    - gpu\n  gpus_per_node: 2  # count\n  min_run_time: 61  # minutes\n  email_file: \"email/multinode_gpu_fragmentation_2.txt\"\n  admin_emails:\n    - alerts-jobs-aaaalegbihhpknikkw2fkdx6gi@princetonrc.slack.com\n    - halverson@princeton.edu\n</code></pre>"},{"location":"alert/gpu_fragmentation/#usage","title":"Usage","text":"<pre><code>$ python job_defense_shield.py --multinode-gpu-fragmentation\n</code></pre>"},{"location":"alert/gpu_fragmentation/#cron","title":"cron","text":""},{"location":"alert/gpu_model_too_powerful/","title":"GPU Model Too Powerful","text":"<p>This alert is used to identify jobs that could have ran on less powerful GPUs. For example, it can find jobs that ran on NVIDIA H100 GPUs but could have used the less powerful L40S GPUs or MIG. The GPU utilization, CPU/GPU memory usage, and number of allocated CPU-cores is taken into account when identifying jobs.</p>"},{"location":"alert/gpu_model_too_powerful/#report","title":"Report","text":"<pre><code>$ python job_defense_shield.py --gpu-model-too-powerful\n\n          GPU Model Too Powerful         \n-----------------------------------------\n User   GPU-Hours  Jobs  JobID    email90\n-----------------------------------------\nyw6760    1.1       1   61122477    2\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/#email-message","title":"Email Message","text":"<pre><code>Hello Alan (u12345),\n\nBelow are jobs that ran on an A100 GPU on Della in the past 10 days:\n\n   JobID    User  GPU-Util GPU-Mem-Used CPU-Mem-Used  Hours\n  60984405 aturing   9%        2 GB         3 GB      3.4  \n  60984542 aturing   8%        2 GB         3 GB      3.0  \n  60989559 aturing   8%        2 GB         3 GB      2.8  \n\nThe jobs above have a low GPU utilization and they use less than 10 GB of GPU\nmemory and less than 32 GB of CPU memory. Such jobs could be run on the MIG\nGPUs. A MIG GPU has 1/7th the performance and memory of an A100. To run on a\nMIG GPU, add the \"partition\" directive to your Slurm script:\n\n  #SBATCH --nodes=1\n  #SBATCH --ntasks=1\n  #SBATCH --cpus-per-task=1\n  #SBATCH --gres=gpu:1\n  #SBATCH --partition=mig\n\nFor interactive sessions use, for example:\n\n  $ salloc --nodes=1 --ntasks=1 --time=1:00:00 --gres=gpu:1 --partition=mig\n\nIf you are using Jupyter OnDemand then set the \"Node type\" to \"mig\" when\ncreating the session.\n\nBy running jobs on the MIG GPUs you will experience shorter queue times and\nyou will help keep A100 GPUs free for jobs that need them. For more info:\n\n  https://researchcomputing.princeton.edu/systems/della#gpus\n\nAs an alternative to MIG, you may consider trying to improve the GPU\nutilization of your code. A good target value is greater than 50%. Consider\nwriting to the mailing list of the software that you are using or attend\nan in-person Research Computing help session:\n\n  https://researchcomputing.princeton.edu/support/help-sessions\n\nFor general information about GPU computing at Princeton:\n\n  https://researchcomputing.princeton.edu/support/knowledge-base/gpu-computing\n\nReplying to this automated email will open a support ticket with Research\nComputing.\n</code></pre> <p>The example alert provided in <code>alert/gpu_model_too_powerful.py</code> </p>"},{"location":"alert/gpu_model_too_powerful/#configuration-file","title":"Configuration File","text":"<p>Here is an example configuration file (<code>config.yaml</code>) entry for the MIG alert:</p> <pre><code>gpu-model-too-powerful:\n  clusters:\n    - della\n  partitions:\n    - gpu\n  min_run_time: 60        # minutes\n  num_cores_threshold: 1  # count\n  num_gpus_threshold: 1   # count\n  gpu_util_threshold: 15  # percent\n  gpu_mem_threshold: 10   # GB\n  cpu_mem_threshold: 32   # GB\n  email_file: \"alert/mig.py\"\n  excluded_users:\n    - aturing\n    - einstein\n</code></pre> <p><code>min_run_time</code> is the minimum run time of the job for it to be considered. Jobs that did not run longer than this limit will be ignored. The default is 60 minutes.</p> <p><code>num_cores_threshold</code> is the number of CPU-cores. For instance, if a job requires a large number of of CPU-cores than it is exempt from MIG. The default value is 1 CPU-core.</p> <p><code>num_gpus_threshold</code>: The number of allocated GPUs to be considered by the alert.</p> <p><code>gpu_util_threshold</code> is the GPU utilization as available from <code>nvidia-smi</code>. Jobs with a GPU utilization of less or equal to this value will be included. The default value is 15%.</p> <p>Some institutions provide a range of MIG instances (e.g., not all H100 or A100 GPUs are converted to 7 MIG instances). In this case you will need to modify the example to find your case. Note that you can make multiple MIG alerts to handle your situation.</p>"},{"location":"alert/gpu_model_too_powerful/#main","title":"Main","text":"<pre><code>if args.mig:\n    alerts = [alert for alert in cfg.keys() if \"should-be-using-mig\" in alert]\n    for alert in alerts:\n        mig = MultiInstanceGPU(df,\n                               days_between_emails=args.days,\n                               violation=\"should_be_using_mig\",\n                               vpath=args.files,\n                               subject=\"Consider Using the MIG GPUs on Della\",\n                               **cfg[alert])\n        if args.email and is_today_a_work_day():\n            mig.send_emails_to_users()\n        s += mig.generate_report_for_admins(\"Could Have Been MIG Jobs\")\n</code></pre>"},{"location":"alert/gpu_model_too_powerful/#usage","title":"Usage","text":"<p>Send emails to users with jobs that could have used the MIG GPUs instead of full GPUs:</p> <pre><code>$ python job_defense_shield.py --gpu-model-too-powerful --clusters=della --partition=gpu --days=7 --email\n</code></pre> <p>Exactly the same as above:</p> <pre><code>$ python job_defense_shield.py --gpu-model-too-powerful -M della -r gpu --email\n</code></pre>"},{"location":"alert/low_cpu_util/","title":"Low CPU Utilization","text":"<p>This option lists the users with the most CPU-hours along with their mean CPU efficiencies. The CPU efficiency is weighted by the number of CPU-cores per job. Jobs with 0% utilization on a node are ignored since they are captured by another alert.</p> <p>This alert is important because it enables system administrators to identify users that are using the most resources in an inefficient way.</p>"},{"location":"alert/low_cpu_util/#report-for-system-administrators","title":"Report for System Administrators","text":"<p>Here is an example of the report:</p> <pre><code>$ python job_defense_shield.py --low-cpu-efficiency\n\n                                       Low GPU Efficiencies                                      \n-------------------------------------------------------------------------------------------------\n    user  cluster partition  gpu-hours  proportion(%)  eff(%)  jobs  interactive  cores  coverage\n-------------------------------------------------------------------------------------------------\n1  u76174   della     gpu      3791          20          29     58        0        1.2      1.0  \n3  u64732   della     gpu      3201          17          50     43        0        1.0      1.0  \n4  u13301   della     gpu      2281          12          43     35        0        8.0      1.0  \n</code></pre> <p>The table lists users from the most GPU-hours to the least. The GPU efficiency is listed.</p> <p>To receive the report by email:</p> <pre><code>$ python job_defense_shield --low-cpu-efficency --emain-admin\n</code></pre> <p>Specify your email in the <code>admin_emails</code> in the configuration file entry for this alert.</p>"},{"location":"alert/low_cpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <p>Minimal configuration for generating reports only (not sending emails to users):</p> <pre><code>low-cpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n</code></pre> <p>This configuration can be used for reports and sending emails:</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n  eff_thres_pct: 15         # percent\n  absolute_thres_hours: 50  # gpu-hours\n  eff_target_pct: 50        # percent\n  email_file: \"email/low_gpu_efficiency.txt\"\n  admin_emails:\n    - admin@university.edu\n    - admin@princeton.edu\n    - sysadmin@princeton.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. One cluster name per alert. Use multiple <code>zero-util-gpu-hours</code> alerts for multiple clusters.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. The number of GPU-hours is summed over all partitions. It most cases it is better to create a separate alert for each partition.</p> </li> <li> <p><code>eff_thres_pct</code>: Efficiency threshold percentage. Users with a <code>eff_thres_pct</code> os less than or equal to this value will receive an email. plus more</p> </li> <li> <p><code>absolute_thres_hours</code>: A user must have allocated more than this number of GPU-hours to be considered to receive an email.</p> </li> <li> <p><code>eff_target_pct</code>: The target value for GPU utilization that users should strive for. It is only used in emails. This value can be referenced as the tag <code>&lt;TARGET&gt;</code> in email messages (see <code>email/low_gpu_efficiencies.txt</code>).</p> </li> <li> <p><code>email_file</code>: The file used as a email body. This file must be found in the <code>email-files-path</code> setting in <code>config.yaml</code>. Learn more about writing custom emails.</p> </li> </ul> <p>Below is a full set of parameters:</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n  eff_thres_pct: 15         # percent\n  proportion_thres_pct: 2   # percent\n  absolute_thres_hours: 50  # gpu-hours\n  eff_target_pct: 50        # percent\n  num_top_users: 15         # count\n  min_run_time: 30          # minutes\n  email_file: \"email/low_gpu_efficiency.txt\"\n  excluded_users:\n    - aturing\n    - einstein\n  admin_emails:\n    - alerts-jobs-aaaalegbihhpknikkw2fkdx6gi@princetonrc.slack.com\n    - halverson@princeton.edu\n    - msbc@princeton.edu\n</code></pre> <ul> <li> <p><code>num_top_users</code>: After sorting all users by GPU-hours, only consider the top <code>num_top_users</code> for all remaining calculations and emails. This is used to limit the number of users that receive emails and appear in reports.</p> </li> <li> <p><code>min_run_time</code>: (Optional) The number of minutes that a job must have ran to be considered. This can be used to exclude test jobs and experimental jobs. The default is 0.</p> </li> <li> <p><code>proportion_thres_pct</code>: A user must being using this proportion of the total GPU-hours (as a percentage) in order to be sent an email. For example, setting this to 2 will excluded all users that are using less than 2% of the total GPU-hours.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails. These users will still appear in reports for system administrators when <code>--report</code> is used.</p> </li> <li> <p><code>user_emails_bcc</code>: (Optional) The emails sent to users will also be sent to these administator emails. This applies when the <code>--email</code> option is used.</p> </li> <li> <p><code>report_emails</code>: (Optional) Reports will be sent to these administator emails. This applies when the <code>--report</code> option is used.</p> </li> </ul>"},{"location":"alert/low_cpu_util/#how-to-write-the-email-file","title":"How to Write the Email File","text":"<p>Below is the email message that is generated by the template in <code>email/low_gpu_efficiencies.txt</code>:</p> <pre><code>Dear Alan,\n\nYou have a loew gpu efficy.\n</code></pre> <p>You can modified the file as you like. The tags that can be used in the email message are:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting that will be generated based on the choice of <code>greeting_method</code> in <code>config.yaml</code>. An example is \"Hello Alan (aturing),\".</li> <li><code>&lt;CLUSTER&gt;</code>: The name of the cluster as defined in <code>config.yaml</code>.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions as defined for the alert in <code>config.yaml</code>.</li> <li><code>&lt;TABLE&gt;</code>: A table of jobs for the user.</li> <li><code>&lt;JOBSTATS&gt;</code>: A line showing how to run the <code>jobstats</code> command on one of the jobs of the user. An example is <code>$ jobstats 1234567</code>.</li> </ul>"},{"location":"alert/low_cpu_util/#usage","title":"Usage","text":"<p>Generate a report of the top users are their CPU efficiencies:</p> <pre><code>$ python job_defense_shield.py --low-cpu-efficiency\n</code></pre> <p>Same as above but over the past month:</p> <pre><code>$ python job_defense_shield.py --low-cpu-efficiency --days=30\n</code></pre> <p>Send emails to users with low GPU efficiencies over the past 7 days:</p> <pre><code>$ python job_defense_shield.py --low-cpu-efficiency --email\n</code></pre> <p>Same as above but only pull data for a specific cluster and partition:</p> <pre><code>$ python job_defense_shield.py --low-cpu-efficiency --email -M traverse -r gpu\n</code></pre>"},{"location":"alert/low_cpu_util/#cron","title":"cron","text":"<p>It is recommended to run this alert with a time window of 7 days:</p> <pre><code>PY=/home/sysadmin/.conda/envs/jds-env/bin\nJDS=/homem/sysadmin/bin/job_defense_shield\nMYLOG=${JDS}/log\n\n0 9 * * * ${PY}/python ${JDS}/job_defense_shield.py --low-cpu-efficiency --email &gt; ${MYLOG}/low_cpu_efficiency.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/low_cpu_util/#troubleshooting","title":"Troubleshooting","text":"<p>You must have a <code>low-cpu-efficiency</code> entry in <code>config.yaml</code> for this alert to work.</p>"},{"location":"alert/low_cpu_util/#related-alerts","title":"Related Alerts","text":"<p>See low CPU utilization</p>"},{"location":"alert/low_gpu_util/","title":"Low GPU Utilization","text":"<p>This alert finds jobs with low GPU utilization. It ignores jobs with GPUs at 0% utilization since the zero GPU utiliation alert catching those cases.</p> <p>This alert is important because it enables system administrators to identify users on the cluster that are using a large amounts of GPU-hours at low utilization.</p> <p>Here is an example of the report:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiencies\n\n                                       Low GPU Efficiencies                                      \n-------------------------------------------------------------------------------------------------\n    user  cluster partition  gpu-hours  proportion(%)  eff(%)  jobs  interactive  cores  coverage\n-------------------------------------------------------------------------------------------------\n1  u76174   della     gpu      3791          20          29     58        0        1.2      1.0  \n3  u64732   della     gpu      3201          17          50     43        0        1.0      1.0  \n4  u13301   della     gpu      2281          12          43     35        0        8.0      1.0  \n</code></pre> <p>The table lists users from the most GPU-hours to the least. The GPU efficiency is listed.</p>"},{"location":"alert/low_gpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <p>Minimal configuration for generating reports only (not sending emails to users):</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n</code></pre> <p>This configuration can be used for reports and sending emails:</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n  eff_thres_pct: 15         # percent\n  absolute_thres_hours: 50  # gpu-hours\n  eff_target_pct: 50        # percent\n  email_file: \"email/low_gpu_efficiency.txt\"\n  admin_emails:\n    - admin@university.edu\n    - admin@princeton.edu\n    - sysadmin@princeton.edu\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. One cluster name per alert. Use multiple <code>zero-util-gpu-hours</code> alerts for multiple clusters.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. The number of GPU-hours is summed over all partitions. It most cases it is better to create a separate alert for each partition.</p> </li> <li> <p><code>eff_thres_pct</code>: Efficiency threshold percentage. Users with a <code>eff_thres_pct</code> os less than or equal to this value will receive an email. plus more</p> </li> <li> <p><code>absolute_thres_hours</code>: A user must have allocated more than this number of GPU-hours to be considered to receive an email.</p> </li> <li> <p><code>eff_target_pct</code>: The target value for GPU utilization that users should strive for. It is only used in emails. This value can be referenced as the tag <code>&lt;TARGET&gt;</code> in email messages (see <code>email/low_gpu_efficiencies.txt</code>).</p> </li> <li> <p><code>email_file</code>: The file used as a email body. This file must be found in the <code>email-files-path</code> setting in <code>config.yaml</code>. Learn more about writing custom emails.</p> </li> </ul> <p>Below is a full set of parameters:</p> <pre><code>low-gpu-efficiency-1:\n  cluster: della\n  partitions:\n    - gpu\n  eff_thres_pct: 15         # percent\n  proportion_thres_pct: 2   # percent\n  absolute_thres_hours: 50  # gpu-hours\n  eff_target_pct: 50        # percent\n  num_top_users: 15         # count\n  min_run_time: 30          # minutes\n  email_file: \"email/low_gpu_efficiency.txt\"\n  excluded_users:\n    - aturing\n    - einstein\n  admin_emails:\n    - alerts-jobs-aaaalegbihhpknikkw2fkdx6gi@princetonrc.slack.com\n    - halverson@princeton.edu\n    - msbc@princeton.edu\n</code></pre> <ul> <li> <p><code>num_top_users</code>: After sorting all users by GPU-hours, only consider the top <code>num_top_users</code> for all remaining calculations and emails. This is used to limit the number of users that receive emails and appear in reports.</p> </li> <li> <p><code>min_run_time</code>: (Optional) The number of minutes that a job must have ran to be considered. This can be used to exclude test jobs and experimental jobs. The default is 0.</p> </li> <li> <p><code>proportion_thres_pct</code>: A user must being using this proportion of the total GPU-hours (as a percentage) in order to be sent an email. For example, setting this to 2 will excluded all users that are using less than 2% of the total GPU-hours.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails. These users will still appear in reports for system administrators when <code>--report</code> is used.</p> </li> <li> <p><code>user_emails_bcc</code>: (Optional) The emails sent to users will also be sent to these administator emails. This applies when the <code>--email</code> option is used.</p> </li> <li> <p><code>report_emails</code>: (Optional) Reports will be sent to these administator emails. This applies when the <code>--report</code> option is used.</p> </li> </ul>"},{"location":"alert/low_gpu_util/#how-to-write-the-email-file","title":"How to Write the Email File","text":"<p>Below is the email message that is generated by the template in <code>email/low_gpu_efficiencies.txt</code>:</p> <pre><code>Dear Alan,\n\nYou have a loew gpu efficy.\n</code></pre> <p>You can modified the file as you like. The tags that can be used in the email message are:</p> <ul> <li><code>&lt;GREETING&gt;</code>: The greeting that will be generated based on the choice of <code>greeting_method</code> in <code>config.yaml</code>. An example is \"Hello Alan (aturing),\".</li> <li><code>&lt;CLUSTER&gt;</code>: The name of the cluster as defined in <code>config.yaml</code>.</li> <li><code>&lt;PARTITIONS&gt;</code>: A comma-separated list of partitions as defined for the alert in <code>config.yaml</code>.</li> <li><code>&lt;TABLE&gt;</code>: A table of jobs for the user.</li> <li><code>&lt;JOBSTATS&gt;</code>: A line showing how to run the <code>jobstats</code> command on one of the jobs of the user. An example is <code>$ jobstats 1234567</code>.</li> </ul>"},{"location":"alert/low_gpu_util/#usage","title":"Usage","text":"<p>Generate a report of the top users are their GPU efficiencies:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiencies\n</code></pre> <p>Same as above but over the past month:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiencies --days=30\n</code></pre> <p>Send emails to users with low GPU efficiencies over the past 7 days:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiencies --email\n</code></pre> <p>Same as above but only pull data for a specific cluster and partition:</p> <pre><code>$ python job_defense_shield.py --low-gpu-efficiencies --email -M traverse -r gpu\n</code></pre>"},{"location":"alert/low_gpu_util/#cron","title":"cron","text":"<p>It is recommended to run this alert with a time window of 7 days:</p> <pre><code>PY=/home/sysadmin/.conda/envs/jds-env/bin\nJDS=/homem/sysadmin/bin/job_defense_shield\nMYLOG=${JDS}/log\n\n0 9 * * * ${PY}/python ${JDS}/job_defense_shield.py --low-gpu-efficiencices --email &gt; ${MYLOG}/low_gpu_efficiencies.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/low_gpu_util/#troubleshooting","title":"Troubleshooting","text":"<p>You must have a <code>low-gpu-efficiencies</code> entry in <code>config.yaml</code> for this alert to work.</p>"},{"location":"alert/low_gpu_util/#related-alerts","title":"Related Alerts","text":"<p>See low CPU utilization</p>"},{"location":"alert/overview/","title":"Overview","text":"<p>The general idea is to examine all of the job data and send emails to user that are underutilizing the systems.</p> <p>Why is it --partition instead of --partitiions. We use the same name as sacct which is partition.</p> <p>Look at the <code>holidays</code> Python module for writing a custom workday.</p>"},{"location":"alert/overview/#email-greeting","title":"Email Greeting","text":""},{"location":"alert/overview/#grace-period","title":"Grace Period","text":"<p>Email alerts are most effective when sent sparingly. For this reason, the software is designed to ensure that a certain amount of time has passed before the user can receive the notice again. Use the <code>--days</code> option to set the minimum number of days between receiving an email.</p>"},{"location":"alert/overview/#violation-history","title":"Violation History","text":"<p>When a user receives an email report, their file is updated with the jobs.</p>"},{"location":"alert/overview/#email-mode","title":"<code>email</code> mode","text":"<p>Use the <code>--email</code> option to send emails to the users for a given alert.</p>"},{"location":"alert/overview/#report-mode","title":"<code>report</code> mode","text":"<p>Use the <code>--report</code> option to send a report for the given alert to system administrators.</p>"},{"location":"alert/overview/#check-mode","title":"<code>check</code> mode","text":"<p>Use the <code>--check</code> to see which users have recevied alerts and when they received them.</p>"},{"location":"alert/overview/#performance-tip","title":"Performance Tip","text":"<p>If you alerts only needs data for specific clusters and partitions then:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --days=7 --clusters=della --partition=gpu,llm --email\n</code></pre> <p>The <code>--clusters</code> and <code>--partition</code> options are passed through to <code>sacct</code>. This means that less data is queried saving time. If you do not specify these options then everything in the Slurm database is retrieved.</p> <p>You can run multiple alerts at once:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --excess-cpu-memory --could-use-mig --days=7 --email\n</code></pre> <p>Here is an entire report:</p> <pre><code>#!/bin/bash\nPY=\"/home/jdh4/bin/jds-env/bin\"\nBASE=\"/tigress/jdh4/utilities/job_defense_shield\"\nCFG=\"${BASE}/config.yaml\"\n${PY}/python -uB ${BASE}/job_defense_shield.py --days=7 \\\n                                               --config-file=${CFG} \\\n                                               --report \\\n                                               --zero-util-gpu-hours \\\n                                               --mig \\\n                                               --low-xpu-efficiency \\\n                                               --zero-cpu-utilization \\\n                                               --cpu-fragmentation \\\n                                               --gpu-fragmentation \\\n                                               --excessive-time \\\n                                               --excess-cpu-memory \\\n                                               --serial-using-multiple \\\n                                               --longest-queued \\\n                                               --jobs-overview \\\n                                               --utilization-overview \\\n                                               --most-gpus \\\n                                               --most-cores &gt; ${BASE}/log/report.log 2&gt;&amp;1\n</code></pre>"},{"location":"alert/overview/#configuration-file","title":"Configuration File","text":"<p>Show a file with multiple alerts and global settings are the top. Explain that the -- flag determines which alerts described in the configuration file run."},{"location":"alert/overview/#next-steps","title":"Next Steps","text":"<p>At this point you are ready to setup the first alert that actually emails users. The simplest and most broadly applicable alert is  excess run time.</p> <p>They key takeaways are to create your config file. You can have multiple entries for the same alert type.</p>"},{"location":"alert/overview/#configuring-crontab","title":"Configuring Crontab","text":"<p>Setup crontab to automatically call the code.</p>"},{"location":"alert/zero_gpu_util/","title":"GPU-Hours at 0% Utilization","text":"<p>This alert sends emails to users that have consumed GPU-hours at 0% utilization. It can also be used to generate a report of these users for system administrators.</p> <p>Here is an example report:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours\n\n                           Zero Utilization GPU-Hours\n-------------------------------------------------------------------------------\n       User   0%-GPU-Hours  Jobs                     JobID                    \n-------------------------------------------------------------------------------\n1     u20461      397        16           60458831,60460188,60478799,60479839+\n2     u99704      196         8           60552976,60552983,60552984,60552985+\n3     u04204       62        39           60457297,60457395,60457408,60460181+\n4     u39983       32        40           60419086,60419088,60419089,60419090+\n5     u93550       22         6           60423037,60423668,60424743,60425344+\n6     u92847       17         5           60516409,60516469,60516554,60516718+\n7     u18225       17        17           60461780,60467419,60467445,60487739+\n8     u99455        9         4            60475110,60496234,60496390,60554903\n9     u30193        8         2                            60424873,60444734_0\n10    u62696        7        13    60422906,60540828_18,60545878_8,60545878_9+\n-------------------------------------------------------------------------------\n   Cluster: della\nPartitions: gpu, llm\n     Start: Thu Sept 1, 2025 at 08:00 AM\n       End: Thu Sept 8, 2025 at 12:37 PM\n</code></pre> <p>The table above shows that user <code>u20461</code> consumed 397 GPU-hours at 0% utilization. Four of the sixteen JobID's are shown.</p>"},{"location":"alert/zero_gpu_util/#configuration-file","title":"Configuration File","text":"<p>Below is an example entry for <code>config.yaml</code>:</p> <pre><code>zero-util-gpu-hours:\n  cluster: della\n  partitions:\n    - gpu\n    - llm\n  min_run_time: 0               # minutes\n  gpu_hours_threshold_user: 24  # hours\n  gpu_hours_threshold_admin: 0  # hours\n  max_num_jobid: 4              # count\n  excluded_users:\n    - u12345\n    - u98765\n  user_emails_bcc:\n    - extra@institution.xyz\n  report_emails:\n    - admin@institution.xyz\n</code></pre> <p>The parameters are explained below:</p> <ul> <li> <p><code>cluster</code>: Specify the cluster name as it appears in the Slurm database. One cluster name per alert. Use multiple <code>zero-util-gpu-hours</code> alerts for multiple clusters.</p> </li> <li> <p><code>partitions</code>: Specify one or more Slurm partitions. The number of GPU-hours is summed over all partitions. Use multiple alerts to change this behavior.</p> </li> <li> <p><code>min_run_time</code>: Minutes number of minutes that a job must have ran to be considered.</p> </li> <li> <p><code>gpu_hours_threshold_user</code>: Only users with greater than or equal to this number of GPU-hours at 0% utilization will receive an email. One should adjust this value based on the choice of the <code>--days</code> option. For <code>--days=7</code>, a reasonble choice is <code>gpu_hours_threshold: 25</code>.</p> </li> <li> <p><code>gpu_hours_threshold_admin</code>: Only users with greater than or equal to this number of GPU-hours at 0% utilization will appear in the report for administrators.</p> </li> <li> <p><code>max_num_jobid</code>: Maximum number of JobID's to show for a given user. If the number of jobs per user is greater than this value then a \"+\" character is appended to the end of the list.</p> </li> <li> <p><code>excluded_users</code>: (Optional) List of users to exclude from receiving emails. These users will still appear in reports for system administrators when <code>--report</code> is used.</p> </li> <li> <p><code>user_emails_bcc</code>: (Optional) The emails sent to users will also be sent to these administator emails. This applies when the <code>--email</code> option is used.</p> </li> <li> <p><code>report_emails</code>: (Optional) Reports will be sent to these administator emails. This applies when the <code>--report</code> option is used.</p> </li> </ul> <p>For jobs that allocate multiple GPUs, only the GPU-hours for the GPUs at 0% utilization are included.</p> <p>Below is second example entry for <code>config.yaml</code>:</p> <pre><code>zero-util-gpu-hours:\n  cluster: stellar\n  partitions:\n    - h100\n  min_run_time: 30               # minutes\n  gpu_hours_threshold_user: 24   # hours\n  gpu_hours_threshold_admin: 12  # hours\n  max_num_jobid: 3               # count\n</code></pre> <p>For the configuration above, only jobs that ran for 30 minutes or more are considered. Users will receive an email (when <code>--email-users</code> is used) if they consumed 24 GPU-hours or more at 0% utilization. System administrators will see users in the report (using <code>--email-admins</code>) that consumed 12 GPU-hours or more. The JobID will be shown for up to three jobs per user. Notice that the optional settings (<code>excluded_users</code>, <code>user_emails_bcc</code>, <code>report_emails</code>) are omitted in the YAML entry.</p>"},{"location":"alert/zero_gpu_util/#how-to-write-your-email-file","title":"How to Write Your Email File","text":"<p>You have these quanities available to you:</p> <pre><code>Dear u20461:\n\nOver the past 7 days you have ran 16 jobs on Della that have burnt\n97 GPU-hours at 0% utilization. Here are the jobid's:\n\n    60458831,60460188,60478799,60479839+\n\nPlease investigate the reason for the GPUs not being used.\n</code></pre>"},{"location":"alert/zero_gpu_util/#usage","title":"Usage","text":"<p>Email users about GPU-hours at 0% utilization: </p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --days=7 --email\n</code></pre> <p>Send a report to system administrators by email:</p> <pre><code>$ job_defense_shield --zero-util-gpu-hours --days=7 --report\n</code></pre>"},{"location":"alert/zero_gpu_util/#related-alerts","title":"Related Alerts","text":"<p>If you are looking to automatically jobs running a 0% GPU utilization then see this section.</p> <p>If you are looking for finding users with low but non-zero GPU utilization then see the low GPU utilization alert.</p>"},{"location":"reports/account/","title":"Usage by Slurm Account","text":""},{"location":"reports/overview/","title":"Reports for System Administrators","text":"<p>The general idea to generate a report is to simply add the <code>--report</code> option to an alert. Here is an example:</p> <pre><code>$ job_defense_shield --zero-gpu-util --days=7 --report\n</code></pre> <p>With the appropriate entry in <code>config.yaml</code>:</p> <pre><code>zero-gpu-utilization:\n  file: alert/zero_util_gpu_hours.py\n  clusters:\n    - della\n  partitions:\n    - gpu\n  excluded_users:\n    - aturing\n    - einstein\n</code></pre>"}]}